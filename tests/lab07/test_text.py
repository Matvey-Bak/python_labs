import pytest
import re
from lib.text import normalize, tokenize, count_freq, top_n


class TestNormalize:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ normalize —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏"""
    
    # –ü–ê–†–ê–ú–ï–¢–†–ò–ó–û–í–ê–ù–ù–´–ï –¢–ï–°–¢–´ - –û–°–ù–û–í–ù–´–ï –°–¶–ï–ù–ê–†–ò–ò
    @pytest.mark.parametrize("input_text,expected", [
        # –ë–∞–∑–æ–≤—ã–µ —Å–ª—É—á–∞–∏
        ("–ü—Ä–∏–≤–µ—Ç, –ú–ò–†!", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
        ("Hello!!! World??? Test...", "hello world test"),
        ("PyThOn JaVa Script", "python java script"),
        ("Test123!@# String456$%^", "test123 string456"),
        ("—ë–ª–∫–∞ –Å–∂ —ë–∂–∏–∫", "–µ–ª–∫–∞ –µ–∂ –µ–∂–∏–∫"),
        
        # –ì—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏
        ("", ""),
        ("   ", ""),
        ("!!! ??? ...", ""),
        ("  test  string  ", "test string"),
        ("TEST\nDATA\tINFO", "test data info"),
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        ("hello\tworld\npython\rjava", "hello world python java"),
        ("hello    world   python", "hello world python"),
    ])
    def test_normalize_basic_cases(self, input_text, expected):
        """–ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤"""
        assert normalize(input_text) == expected
    
    @pytest.mark.parametrize("text,casefold,yo2e,expected", [
        # –ö–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        ("–ü—Ä–∏–≤–µ—Ç –ú–ò–†", True, True, "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
        ("–ü—Ä–∏–≤–µ—Ç –ú–ò–†", False, True, "–ü—Ä–∏–≤–µ—Ç –ú–ò–†"),
        ("—ë–ª–∫–∞ –Å–ñ", True, True, "–µ–ª–∫–∞ –µ–∂"),
        ("—ë–ª–∫–∞ –Å–ñ", True, False, "—ë–ª–∫–∞ —ë–∂"),
        ("Test Case", False, False, "Test Case"),
        ("Test Case", True, False, "test case"),
        
        # –ì—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        ("", True, True, ""),
        ("!!!", False, False, "!!!"),
        ("  —ë  ", False, True, "–µ"),
    ])
    def test_normalize_parameter_combinations(self, text, casefold, yo2e, expected):
        """–ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        result = normalize(text, casefold=casefold, yo2e=yo2e)
        assert result == expected
    
    def test_normalize_only_special_chars(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å—Ç—Ä–æ–∫–∞ —Ç–æ–ª—å–∫–æ –∏–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏"""
        text = "!!! @@@ ### $$$"
        result = normalize(text)
        expected = ""
        assert result == expected
    
    def test_normalize_only_whitespace(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å—Ç—Ä–æ–∫–∞ —Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        text = "   \t\n\r   \t\t"
        result = normalize(text)
        expected = ""
        assert result == expected
    
    def test_normalize_large_text_performance(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–æ–º —Ç–µ–∫—Å—Ç–µ"""
        large_text = "test " * 1000
        result = normalize(large_text)
        assert len(result) == len("test") * 1000 + 999
        assert result.count("  ") == 0
    
    def test_normalize_yo_replacement_comprehensive(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –≤—Å–µ —Å–ª—É—á–∞–∏ –∑–∞–º–µ–Ω—ã –±—É–∫–≤—ã '—ë'"""
        text = "—ë –Å —ë—ë –Å–Å –º—ë–¥ –ú–Å–î —ë–ª–∫–∞ –Å–ñ–ò–ö"
        result = normalize(text)
        expected = "–µ –µ –µ–µ –µ–µ –º–µ–¥ –º–µ–¥ –µ–ª–∫–∞ –µ–∂–∏–∫"
        assert result == expected
    
    def test_normalize_unicode_special_cases(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ Unicode —Å–∏–º–≤–æ–ª—ã"""
        # –ù–µ–º–µ—Ü–∫–∏–µ —É–º–ª–∞—É—Ç—ã
        text = "√Ñ √ñ √ú √ü"
        result = normalize(text)
        expected = text.casefold()
        assert result == expected
    
    def test_normalize_input_validation(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        with pytest.raises(TypeError):
            normalize(None)
        with pytest.raises(TypeError):
            normalize(123)
        with pytest.raises(TypeError):
            normalize([])


class TestTokenize:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ tokenize —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏"""
    
    @pytest.mark.parametrize("text,expected", [
        # –ë–∞–∑–æ–≤—ã–µ —Å–ª—É—á–∞–∏
        ("hello world python", ["hello", "world", "python"]),
        ("hello, world! python?", ["hello", "world", "python"]),
        ("test123 string456 data789", ["test123", "string456", "data789"]),
        ("Hello, World! Test123... Data_456", ["hello", "world", "test123", "data456"]),
        
        # –ì—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏
        ("", []),
        ("   ", []),
        ("!!! @@@ ###", []),
        ("single", ["single"]),
        ("  hello  world  ", ["hello", "world"]),
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        ("test\nstring\tdata", ["test", "string", "data"]),
        ("word1, word2! word3?", ["word1", "word2", "word3"]),
    ])
    def test_tokenize_basic_cases(self, text, expected):
        """–ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤"""
        result = tokenize(text)
        assert result == expected
    

    def test_tokenize_only_punctuation(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å—Ç—Ä–æ–∫–∞ —Ç–æ–ª—å–∫–æ –∏–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        text = "!!! @@@ ### $$$ %%% ^^^"
        result = tokenize(text)
        expected = []
        assert result == expected
    
    def test_tokenize_mixed_whitespace_comprehensive(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –≤—Å–µ –≤–∏–¥—ã –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        text = "word1\tword2\nword3\rword4\u00A0word5"
        result = tokenize(text)
        expected = ["word1", "word2", "word3", "word4", "word5"]
        assert result == expected
    
    def test_tokenize_unicode_text(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: Unicode —Å–∏–º–≤–æ–ª—ã"""
        text = "–ü—Ä–∏–≤–µ—Ç ‰∏ñÁïå  Hello caf√© na√Øve"
        result = tokenize(text)
        expected = ["–ø—Ä–∏–≤–µ—Ç", "‰∏ñÁïå", "hello", "caf√©", "na√Øve"]
        assert result == expected
    
    def test_tokenize_large_text_performance(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–æ–º —Ç–µ–∫—Å—Ç–µ"""
        large_text = "token " * 5000
        tokens = tokenize(large_text)
        assert len(tokens) == 5000
        assert all(token == "token" for token in tokens)
    
    def test_tokenize_preserve_underscores(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–π"""
        text = "test_variable snake_case data_set"
        result = tokenize(text)
        expected = ["test_variable", "snake_case", "data_set"]
        assert result == expected
    
    def test_tokenize_input_validation(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        with pytest.raises(TypeError):
            tokenize(None)
        with pytest.raises(TypeError):
            tokenize(123)


class TestCountFreq:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ count_freq —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏"""
    
    # –ü–ê–†–ê–ú–ï–¢–†–ò–ó–û–í–ê–ù–ù–´–ï –¢–ï–°–¢–´ - –û–°–ù–û–í–ù–´–ï –°–¶–ï–ù–ê–†–ò–ò
    @pytest.mark.parametrize("tokens,expected", [
        # –ë–∞–∑–æ–≤—ã–µ —Å–ª—É—á–∞–∏
        (["hello", "world", "hello", "python"], {"hello": 2, "world": 1, "python": 1}),
        (["a", "b", "c", "d"], {"a": 1, "b": 1, "c": 1, "d": 1}),
        (["word", "word", "word", "word"], {"word": 4}),
        (["test"], {"test": 1}),
        
        # –ì—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏
        ([], {}),
        (["", "word", "", "test"], {"": 2, "word": 1, "test": 1}),
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏
        (["Hello", "hello", "HELLO"], {"Hello": 1, "hello": 1, "HELLO": 1}),
        (["caf√©", "caf√©", "na√Øve"], {"caf√©": 2, "na√Øve": 1}),
    ])
    def test_count_freq_basic_cases(self, tokens, expected):
        """–ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤"""
        result = count_freq(tokens)
        assert result == expected
    
    # –û–¢–î–ï–õ–¨–ù–´–ï –ü–†–û–í–ï–†–ö–ò - –°–ü–ï–¶–ò–§–ò–ß–ï–°–ö–ò–ï –ò –°–õ–û–ñ–ù–´–ï –°–õ–£–ß–ê–ò
    def test_count_freq_special_characters_in_tokens(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ —Ç–æ–∫–µ–Ω–∞—Ö"""
        tokens = ["test_123", "test-456", "test.789", "var@name"]
        result = count_freq(tokens)
        expected = {"test_123": 1, "test-456": 1, "test.789": 1, "var@name": 1}
        assert result == expected
    
    def test_count_freq_unicode_comprehensive(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ Unicode —Å–ª—É—á–∞–∏"""
        tokens = ["caf√©", "caf√©", "na√Øve", "caf√©", "üöÄ", "üöÄ", "‰∏ñÁïå"]
        result = count_freq(tokens)
        expected = {"caf√©": 3, "na√Øve": 1, "üöÄ": 2, "‰∏ñÁïå": 1}
        assert result == expected
    
    def test_count_freq_large_dataset_performance(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö"""
        tokens = ["word"] * 1000 + ["test"] * 500 + ["data"] * 250
        result = count_freq(tokens)
        assert result["word"] == 1000
        assert result["test"] == 500
        assert result["data"] == 250
        assert len(result) == 3
    
    def test_count_freq_preserve_order_independence(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –ø–æ—Ä—è–¥–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        tokens1 = ["a", "b", "a", "c", "b", "a"]
        tokens2 = ["b", "a", "c", "a", "a", "b"]
        result1 = count_freq(tokens1)
        result2 = count_freq(tokens2)
        assert result1 == result2 == {"a": 3, "b": 2, "c": 1}
    
    def test_count_freq_input_validation(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        with pytest.raises(TypeError):
            count_freq(None)
        with pytest.raises(TypeError):
            count_freq("not a list")
        with pytest.raises(TypeError):
            count_freq([1, 2, 3])  


class TestTopN:
    
    @pytest.mark.parametrize("freq,n,expected", [
        ({"hello": 5, "world": 3, "python": 7, "java": 2}, 3, [("python", 7), ("hello", 5), ("world", 3)]),
        ({"a": 1, "b": 2, "c": 3, "d": 4, "e": 5, "f": 6}, 5, [("f", 6), ("e", 5), ("d", 4), ("c", 3), ("b", 2)]),
        ({"test": 42}, 1, [("test", 42)]),
        
        ({}, 5, []),
        ({"a": 1, "b": 2}, 10, [("b", 2), ("a", 1)]),
        ({"a": 1, "b": 2, "c": 3}, 0, []),
        ({"a": 1, "b": 2}, -1, []),
        
        ({"banana": 3, "apple": 3, "cherry": 3, "date": 2}, 4, [("apple", 3), ("banana", 3), ("cherry", 3), ("date", 2)]),
        ({"z": 1, "y": 2, "x": 3}, 1, [("x", 3)]),
    ])
    def test_top_n_basic_cases(self, freq, n, expected):
        """–ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤"""
        result = top_n(freq, n)
        assert result == expected
    
    @pytest.mark.parametrize("freq,expected", [
        ({"a": 1, "b": 2, "c": 3, "d": 4, "e": 5, "f": 6}, [("f", 6), ("e", 5), ("d", 4), ("c", 3), ("b", 2)]),
        ({"single": 10}, [("single", 10)]),
        ({}, []),
    ])
    def test_top_n_default_parameter(self, freq, expected):
        """–ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        result = top_n(freq)
        assert result == expected
    

    def test_top_n_complex_alphabetical_sort(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–ª–æ–∂–Ω–∞—è –∞–ª—Ñ–∞–≤–∏—Ç–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Å —É—á–µ—Ç–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞"""
        freq = {"apple": 2, "Apple": 2, "banana": 2, "Banana": 2}
        result = top_n(freq, 4)
        expected = [("Apple", 2), ("Banana", 2), ("apple", 2), ("banana", 2)]
        assert result == expected
    
    def test_top_n_preserve_order_same_frequency(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —á–∞—Å—Ç–æ—Ç–∞—Ö"""
        freq = {"zebra": 5, "apple": 5, "cherry": 5, "banana": 5}
        result = top_n(freq, 4)
        expected = [("apple", 5), ("banana", 5), ("cherry", 5), ("zebra", 5)]
        assert result == expected

        assert [word for word, freq in result] == ["apple", "banana", "cherry", "zebra"]
    
    def test_top_n_mixed_frequencies_complex_sort(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–º–µ—à–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π"""
        freq = {"delta": 1, "alpha": 3, "gamma": 1, "beta": 3, "epsilon": 2}
        result = top_n(freq, 5)
        expected = [("alpha", 3), ("beta", 3), ("epsilon", 2), ("delta", 1), ("gamma", 1)]
        assert result == expected
    
    def test_top_n_unicode_sorting(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ Unicode —Å–∏–º–≤–æ–ª–æ–≤"""
        freq = {"‰∏ñÁïå": 3, "hello": 3, "caf√©": 2, "üöÄ": 4}
        result = top_n(freq, 4)
        # –î–æ–ª–∂–Ω—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø–æ —á–∞—Å—Ç–æ—Ç–µ, –∞ –ø—Ä–∏ —Ä–∞–≤–Ω–æ–π - –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
        expected = [("üöÄ", 4), ("hello", 3), ("‰∏ñÁïå", 3), ("caf√©", 2)]
        assert result == expected
    
    def test_top_n_stability_multiple_calls(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö"""
        freq = {"a": 1, "b": 1, "c": 1, "d": 1, "e": 1}
        
        for i in range(10):
            result = top_n(freq, 5)
            expected = [("a", 1), ("b", 1), ("c", 1), ("d", 1), ("e", 1)]
            assert result == expected
    
    def test_top_n_input_validation(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        with pytest.raises(TypeError):
            top_n(None, 5)
        with pytest.raises(TypeError):
            top_n("not a dict", 5)
        with pytest.raises(TypeError):
            top_n({"a": 1}, "not an int")


class TestIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –ø–æ–ª–Ω–æ–≥–æ pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
    
    @pytest.mark.parametrize("text,expected_top", [
        ("Hello hello world! World python. Python Python!", [("python", 3), ("hello", 2), ("world", 2)]),
        ("Test!!! Test... Data? Data! Info; Info: Info", [("info", 3), ("data", 2), ("test", 2)]),
        ("Caf√© caf√© na√Øve Na√Øve test Test", [("caf√©", 2), ("na√Øve", 2), ("test", 2)]),
    ])
    def test_integration_pipeline_parametrized(self, text, expected_top):
        """–ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –ø–æ–ª–Ω–æ–≥–æ pipeline"""
        tokens = tokenize(text)
        freq = count_freq(tokens)
        top_words = top_n(freq, 3)
        assert top_words == expected_top
    
    def test_integration_complex_scenario(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–ª–æ–∂–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π —Å mixed case –∏ —Ä–∞–∑–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏"""
        text = """
        Python python JAVA java C++ c++ 
        JavaScript javascript TypeScript typescript
        Python is great! Java is good.
        """
        tokens = tokenize(text)
        freq = count_freq(tokens)
        top_words = top_n(freq, 5)
        
        assert top_words[0][0] == "python"
        assert top_words[0][1] == 3
        assert "java" in [word for word, freq in top_words]
    
    def test_integration_empty_text(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ –≤–µ—Å—å pipeline"""
        text = ""
        tokens = tokenize(text)
        freq = count_freq(tokens)
        top_words = top_n(freq, 5)
        
        assert tokens == []
        assert freq == {}
        assert top_words == []
    
    def test_integration_special_characters_only(self):
        """–û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –∏–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        text = "!!! @@@ ### $$$ %%% ^^^ &&&"
        tokens = tokenize(text)
        freq = count_freq(tokens)
        top_words = top_n(freq, 5)
        
        assert tokens == []
        assert freq == {}
        assert top_words == []