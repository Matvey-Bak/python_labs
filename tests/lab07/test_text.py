import pytest  # type: ignore
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –≤ PYTHONPATH –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
src_path = os.path.join(project_root, "src")
if src_path not in sys.path:
    sys.path.insert(0, src_path)

from lib.text import normalize, tokenize, count_freq, top_n


class TestNormalize:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ normalize"""

    @pytest.mark.parametrize(
        "input_text,expected",
        [
            ("–ü—Ä–∏–≤–µ—Ç, –ú–ò–†!", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
            ("Hello!!! World???", "hello world"),
            ("PyThOn JaVa", "python java"),
            ("—ë–ª–∫–∞ –Å–∂", "–µ–ª–∫–∞ –µ–∂"),
            ("", ""),
            ("   ", ""),
            ("!!!", ""),
            ("  test  string  ", "test string"),
        ],
    )
    def test_normalize_basic(self, input_text, expected):
        assert normalize(input_text) == expected

    @pytest.mark.parametrize(
        "text,casefold,yo2e,expected",
        [
            ("–ü—Ä–∏–≤–µ—Ç –ú–ò–†", True, True, "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
            ("—ë–ª–∫–∞ –Å–ñ", True, True, "–µ–ª–∫–∞ –µ–∂"),
            ("Test Case", True, False, "test case"),
        ],
    )
    def test_normalize_params(self, text, casefold, yo2e, expected):
        result = normalize(text, casefold=casefold, yo2e=yo2e)
        assert result == expected

    def test_normalize_special_cases(self):
        assert normalize("!!! @@@") == ""
        assert normalize("   \t\n") == ""


class TestTokenize:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ tokenize"""

    @pytest.mark.parametrize(
        "text,expected",
        [
            ("hello world", ["hello", "world"]),
            ("hello, world!", ["hello", "world"]),
            ("test123 string456", ["test123", "string456"]),
            ("", []),
            ("   ", []),
            ("!!!", []),
            ("single", ["single"]),
        ],
    )
    def test_tokenize_basic(self, text, expected):
        assert tokenize(text) == expected

    def test_tokenize_special_cases(self):
        assert tokenize("test\nstring\tdata") == ["test", "string", "data"]
        assert tokenize("–ü—Ä–∏–≤–µ—Ç ‰∏ñÁïå caf√©") == ["–ø—Ä–∏–≤–µ—Ç", "‰∏ñÁïå", "caf√©"]


class TestCountFreq:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ count_freq"""

    @pytest.mark.parametrize(
        "tokens,expected",
        [
            (["hello", "world", "hello"], {"hello": 2, "world": 1}),
            (["a", "b", "c"], {"a": 1, "b": 1, "c": 1}),
            (["word", "word"], {"word": 2}),
            ([], {}),
        ],
    )
    def test_count_freq_basic(self, tokens, expected):
        assert count_freq(tokens) == expected

    def test_count_freq_special(self):
        tokens = ["caf√©", "caf√©", "üöÄ"]
        assert count_freq(tokens) == {"caf√©": 2, "üöÄ": 1}


class TestTopN:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ top_n"""

    @pytest.mark.parametrize(
        "freq,n,expected",
        [
            ({"python": 7, "hello": 5, "world": 3}, 2, [("python", 7), ("hello", 5)]),
            ({"test": 42}, 1, [("test", 42)]),
            ({}, 5, []),
            ({"a": 1, "b": 2}, 10, [("b", 2), ("a", 1)]),
        ],
    )
    def test_top_n_basic(self, freq, n, expected):
        assert top_n(freq, n) == expected

    def test_top_n_alphabetical_sort(self):
        freq = {"zebra": 5, "apple": 5, "cherry": 5}
        result = top_n(freq, 3)
        assert [word for word, _ in result] == ["apple", "cherry", "zebra"]


class TestIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã"""

    @pytest.mark.parametrize(
        "text,expected",
        [
            ("Hello hello world!", [("hello", 2), ("world", 1)]),
            ("Test test data", [("test", 2), ("data", 1)]),
        ],
    )
    def test_pipeline(self, text, expected):
        tokens = tokenize(text)
        freq = count_freq(tokens)
        result = top_n(freq, 3)
        assert result == expected

    def test_empty_pipeline(self):
        text = ""
        tokens = tokenize(text)
        freq = count_freq(tokens)
        result = top_n(freq, 5)
        assert result == []
